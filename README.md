Title: Apply topsis to find best Pretrained model for Text Classification
1. Introduction: 
  Text classification is a fundamental Natural Language Processing (NLP) task used in applications such as sentiment analysis, spam detection, document categorization, and chatbot systems. Multiple pretrained transformer models such as BERT, RoBERTa, DistilBERT, XLNet, and ALBERT provide different performance levels in terms of accuracy, efficiency, inference time, and model size.
  TOPSIS selects the alternative that is closest to the ideal best solution and farthest from the ideal worst solution.
2. Objective:
   The objective of this project is to:
   - Compare multiple pretrained text classification models

  - Apply the TOPSIS method
  
  - Rank the models based on multiple criteria
  
  - Identify the best model
  
  - Visualize the results using tables and graphs
3. Pretrained Models Evaluated:
   The following pretrained models are evaluated:

  1. BERT
  
  2. RoBERTa
  
  3. DistilBERT
  
  4. XLNet
  
  5. ALBERT

5. Input:
   
   <img width="852" height="411" alt="image" src="https://github.com/user-attachments/assets/1ca59486-10e5-4a1e-98fe-ce4700af47c7" />

6. Ouput:
   
   <img width="693" height="273" alt="image" src="https://github.com/user-attachments/assets/7b4bfce4-7d4f-4a89-958d-6e0e92aa52e2" />

7. Visual Results:
   
   <img width="645" height="516" alt="image" src="https://github.com/user-attachments/assets/a413b6d4-3d2d-4059-9c5b-98dde56aebe0" />

8. Live Link:
   https://colab.research.google.com/drive/1AISqD4tb4uDh50Y0LlRy2DPThEbCG2LY#scrollTo=sepU5kQyiAMo

